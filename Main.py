import streamlit as st

# S·ª≠ d·ª•ng ChatOpenAI & OpenAIEmbeddings t·ª´ langchain_openai:
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

# S·ª≠ d·ª•ng FAISS t·ª´ langchain_community:
from langchain_community.vectorstores import FAISS

# S·ª≠ d·ª•ng c√°c th√†nh ph·∫ßn c·ªët l√µi
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document
from langchain_core.prompts import PromptTemplate

# Ho·∫∑c t·∫°m th·ªùi v·∫´n c√≥ th·ªÉ import t·ª´ langchain, 
# nh∆∞ng s·∫Ω b·ªã c·∫£nh b√°o trong phi√™n b·∫£n t∆∞∆°ng lai.

# Nh·∫≠p PromptTemplate d·∫°ng "ChatPromptTemplate" (n·∫øu c·∫ßn):
from langchain.prompts.chat import (
    SystemMessagePromptTemplate, 
    HumanMessagePromptTemplate, 
    ChatPromptTemplate
)
import os
from dotenv import load_dotenv
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

from prompt.prompts import BASE_SYSTEM_PROMPT  # T·ª´ file prompts.py

# 1. Chu·∫©n b·ªã d·ªØ li·ªáu Document
docs = [
    Document(page_content="B·ªánh c·∫£m c√∫m th∆∞·ªùng g√¢y ra tri·ªáu ch·ª©ng s·ªï m≈©i, ho, ƒëau ƒë·∫ßu, m·ªát m·ªèi."),
    Document(page_content="Vi√™m ph·ªïi th∆∞·ªùng g√¢y ra ho, kh√≥ th·ªü, c√≥ khi s·ªët v√† ƒëau ng·ª±c."),
]

# 2. T·∫°o Vector Store (FAISS)
embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)
vectorstore = FAISS.from_documents(docs, embedding_model)
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 3. C·∫•u h√¨nh m√¥ h√¨nh ChatOpenAI
llm = ChatOpenAI(
    model_name="gpt-3.5-turbo",
    temperature=0.7,
    openai_api_key=openai_api_key
)

# 4. ƒê·ªãnh nghƒ©a bi·∫øn my_prompt_template (CH√çNH L√Ä CH·ªñ B·ªä THI·∫æU)
my_prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω y t·∫ø AI.
N·ªôi dung t√†i li·ªáu h·ªó tr·ª£ (context): {context}

Ng∆∞·ªùi d√πng h·ªèi (question): {question}

H√£y ƒë∆∞a ra c√¢u tr·∫£ l·ªùi chi ti·∫øt. Sau ƒë√≥, nh·∫Øc ng∆∞·ªùi d√πng li√™n h·ªá b√°c sƒ© n·∫øu c√≥ b·∫•t k·ª≥ d·∫•u hi·ªáu nghi√™m tr·ªçng n√†o.
"""

prompt = PromptTemplate(
    template=my_prompt_template,
    input_variables=["context", "question"]
)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    chain_type_kwargs={
        "prompt": prompt,
        # M·∫∑c ƒë·ªãnh chain "stuff" s·∫Ω nh√©t t√†i li·ªáu v√†o bi·∫øn 'context' 
        # (Kh·ªõp v·ªõi prompt template n√™u tr√™n)
        "document_variable_name": "context"
    }
)

# 5. Giao di·ªán Streamlit
st.title("ü§ñ Chatbot Y T·∫ø AI")

user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi v·ªÅ s·ª©c kho·∫ª c·ªßa b·∫°n...")

if st.button("G·ª≠i c√¢u h·ªèi"):
    if user_query.strip() == "":
        st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung c√¢u h·ªèi!")
    else:
        with st.spinner("ƒêang x·ª≠ l√Ω..."):
            result = qa_chain.run(user_query)
        st.markdown(f"**C√¢u tr·∫£ l·ªùi:** {result}")
        st.info("ƒê√¢y ch·ªâ l√† khuy·∫øn ngh·ªã. H√£y g·∫∑p b√°c sƒ© ƒë·ªÉ ƒë∆∞·ª£c ch·∫©n ƒëo√°n ch√≠nh x√°c.")
